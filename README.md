# Portfolio Applied Data Science
<h4>Naam: Jesse Huizing <br><br>
Studentnummer: 18053580 <br><br>
Studie: Technische Bedrijfskunde <br><br></h4>

In dit portfolio zal ik bespreken waar ik het afgelopen semester mee bezig ben geweest tijdens de minor applied data science.
In het volgende kopje zijn de verplichte criteria te vinden, zoals de datacamp courses.
<details><summary><h2>Verplichte Criteria</h2></summary>
Om mee te beginnen zal heb ik ongeveer 90% Datacamp courses afgrond. Hieronder zijn de screenshots ervan te vinden:
  <img src="https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Datacamp.png">

</details>

<details><summary><h2>Domain knowledge</h2></summary>
  <h4>Introduction to the subject fiels</h4>
  Bij deze minor was het de bedoeling om aan de slag te gaan met data-analyse, dit werd gedaan met behulp van de programmeertaal 'Python'. Python onderscheid zich, omdat de taal erg goed leesbaar is. De taal is geschikt voor een breed scala aan toepassingen, waaronder webontwikkeling, automatisering, data-analyse en machine learning. Dankzij de grote hoeveelheid beschikbare libraries, modules en frameworks, is het mogelijk om complexe taken uit te voeren zonder veel code te hoeven schrijven. Python wordt daarom ook vaak gebruikt in de academische wereld voor wetenschappelijk onderzoek en in de bedrijfsomgevingen, voor het analyseren van grote goeveelheden data. Kortom, python is enorm veelzijdig en kan op veel verschillende manieren worden toegepast.<br><br>
  Tijdens deze minor is python daarom ook gebruikt om onderzoek te doen, voor zowel het analyseren en aanbieden van vegetarische, als voor het oplossen van een container-stacking probleem. De mogelijkheden van python zijn enorm groot.<br>
  <h4>Termologie</h4>
  Machine learning: een subset van kunstmatige intelligentie waarbij systemen in staat zijn om van data te leren zonder expliciete programmering.<br>
  Supervised learning: een vorm van machine learning waarbij een model wordt getraind met behulp van gekoppelde input- en outputdata. Het doel is om een model te creëren dat nieuwe inputdata kan classificeren of voorspellen op basis van de input-output associaties van de trainingsdata.<br>
  Neurale netwerken: een specifiek type machine learning model dat bestaat uit een netwerk van neurale elementen, die in staat zijn om complexe relaties tussen input- en outputdata te leren<br>
  Overfitting: een situatie waarbij een model te goed is getraind op de trainingsdata, waardoor het niet meer in staat is om nieuwe data correct te classificeren of te voorspellen.<br>
  Underfitting (of onderschattend): een situatie waarbij een machine learning model niet voldoende is getraind op de gegevens en daardoor niet in staat is om een goede voorspelling te doen voor nieuwe data<br>
  Hyperparameters: instellingen van een machine learning model, zoals het aantal lagen of de grootte van de batch, die worden gekozen voordat het model wordt getraind.<br>
  Cross-validation: een methode om het prestatieniveau van een model te evalueren door het model op verschillende delen van de data te trainen en te testen.<br>
  Libraries: een verzameling van functies en methoden die kunnen worden gebruikt in een Python-programma, zoals NumPy, Pandas, en scikit-learn voor machine learning.<br>
  <h4>Literatuur</h4>
  Er zijn ook nog een aantal belangrijke bronnen die ik heb gebruikt tijdens deze minor, dat zijn de volgende:
  Desaraju, N. (2022, 8 8). Hands-On Introduction to Reinforcement Learning in Python. Opgehaald van Medium: https://towardsdatascience.com/hands-on-introduction-to-reinforcement-learning-in-python-da07f7aaca88<br>
  Gupta, A. (2019, 6 7). ML | Reinforcement Learning Algorithm : Python Implementation using Q-learning. Opgehaald van GeeksforGeeks: https://www.geeksforgeeks.org/ml-reinforcement-learning-algorithm-python-implementation-using-q-learning/?ref=rp<br>
  Ntriankos, V. (2022). Optimising the yard layout of Container Terminals. Delft: TU Delft.<br>
  Shih-Chan, T., Jaw-Shen, W., Shen-Long, K., & Flor Melina, P. (sd). Categorized stacking models for import containers in port container terminals. Maritime Economics &amp; Logistics.<br>
  Verma, R., Saikia, S., Khadilkar, H., Agarwal, P., Shroff, G., & Srinivasan, A. (2019, 5 8). A Reinforcement Learning Framework for Container Selection and Ship Load Sequencing in Ports. AAMAS '19: Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2250-2252.<br>
  
  
  </details>
  
<details><summary><h2>Research Project</h2></summary>

Waar ik onder andere veel tijd in heb gestoken, is het research project. Voornamelijk heb ik mijzelf bezig gehouden met het schrijven van de inleiding van beide projecten. Zowel het FoodBoost als het Container project. Daarnaast heb ik ook de hoofd- en deelvragen en de doelstelling geformuleerd voor deze projecten. Ik zal per project toelichten wat ik heb bijgedragen 

  <details><Summary><h2>Foodboost</h2></summary>
In de eerste 6 weken waren wij als groep bezig gegaan met het foodboost project, het research paper is <a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Research%20paper%20FoodBoost.pdf'> hier </a>  
  te vinden. Ik merkte dat mijn groepsgenoten iets op mij voor liepen qua coderen had ik mijzelf voornamelijk bezig gehouden met het research paper. Ik had de hoofdvraag en de daarbij horende deelvragen geformuleerd voor dit project. Omdat ik mensen persoonlijk ken die een vegatarische levensstijl hebben wilde ik mij graag richten op deze doelgroep met dit project. Het was daarom ook belangrijk om alle behoeften van een vegetariër hierin mee te nemen, denk hierbij aan de vitamine B12 en proteïne's. Ook is het belangrijk dat dit dieet wekelijks kan variëren, niet dat een paar vegetarische gerechten het complete dieet vormen. Dit resulteerde in de volgende hoofd- en deelvragen:<br><br>
  
Hoofdvraag:<br><br>
  
* Hoe kunnen we voor 7 oktober 2022 vegetariërs een wekelijks variërend dieet aanbieden dat aan hun verschillende persoonlijke behoeften op het gebied van voedingswaarden en voorkeuren voldoet?<br>

Deelvragen:
  
* Welke behoeften heeft een vegetariër met betrekking tot het dieet?
* Hoe kan er een vegetarisch dieet aangeboden worden zodat deze voldoet aan alle verschillende persoonlijke caloriebehoeften binnen de groep vegetariërs.
* Hoe kan er een vegetarisch dieet worden aangeboden dat wekelijks varieert.
Daarnaast heb ik voor dit project ook de inleiding geschreven. In principe bestond het paper alleen maar uit deze onderdelen omdat we als groep al hadden besloten om ons te gaan richten op het container project
  
  <h4>Evaluation</h4>
  Terugkijkend op dit projecten, met de kennis van nu, merk ik dat we (als groep), maar vooral persoonlijk meer uit dit project hadden kunnen halen. Ik had op dat moment nog niet genoeg kennis over de code en kon mij hierdoor niet volledig inzetten voor het project, voornamlijk Charlie en Richal hadden het voortouw genomen bij het coderen. Als ik dit project nu had kunnen overdoen, dan had ik betere code hebben kunnen schrijven.
  
  <h4>Conclusions</h4>
  Omdat we dit project niet hebben afgerond (en van start zijn gegaan met het containerproject) is het helaas niet gelukt om een conclusie te schrijven en de hoofd-en deelvragen te beantwoorden. Nu ik terugkijk wil ik mijzelf graag nog een keer in dit project verdiepen om de hoofd-en deelvragen te beantwoorden.<br>
    Wat wel was gelukt is het volgende:
    Er waren verschillende modellen gemaakt en getraind voor het voorspellen van voorkeuren van gesimuleerde gebruikers. Dit werd gedaan door het koppelen van gebruikers die dezelfde voorkeuren hadden; Stel als het overgrote deel van de gebruikers kaas en ijsbergsla lekker vinden, dan kan er voor een nieuwe gebruiker die ook van kaas houdt, voorspeld worden dat diegene ook van ijsbergsla zal houden. Dit is gedaan aan de hand van een dataset die bestaat uit gesimuleerde gebruikers. Het uiteindelijke doel was dan ook om ditzelfde resultaat te krijgen met echte gebruikers. Er zou dan aan de hand van een aantal favoriete gerechten een voorspelling worden gedaan voor een nieuw weekmenu.
  
  <h4>Planning</h4>
  Tijdens het project hebben we met een <a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/SCRUM%20Foodboost.png'> scrum board </a> gewerkt.
</details>

<details><Summary><h2>Container</h2></summary>
Voor dit project heb ik ook meegeholpen met het formuleren van de hoofd- en deelvragen, deze zijn wel aangepast naar mate wij verder gingen in dit project. Daarnaast heb ik voor het <a href ="https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Research%20Paper%20Container%20Project%20Groep%204.pdf"> research paper </a>, dat wij uiteindelijk ingeleverd het volgende bijgedragen:<br>
  
* Het schrijven van de inleiding, bestaande uit: een aanleiding, probleemstelling en doelstelling. De eerste opzet van de inleiding is geschreven door Akram<br>
* Het literatuuronderzoek, bestaande uit: indeling terminal, processen containerterminal, reinforcement learning.<br>
* De laatste checks uitvoeren en de opmaak verbeteren op het einde.<br>
  
  <h4>Evaluation</h4>
  
  In dit kopje zal ik bespreken wat ik het afgelopen project heb bijgedragen en welke activiteiten in de toekomst kunnen worden uitgevoerd ten behoeve van het onderzoek en de resultaten. <br><br>
  Toen ik begon aan dit project merkte ik pas hoe moeilijk Reinforcement learning daadwerkelijk was. Ik had niet meer de houvast aan de opdrachten van DataCamp. Ik kon hier in het begin erg moeilijk mee omgaan. Nu ik terugkijk had ik mijzelf graag meer ingelezen in reinforcement learning aan het begin van het project. Met name de literatuur, aangezien er enorm veel literatuur over dit onderwerp was te vinden. Als ik dit zou kunnen overdoen dan had ik dit ook zeker gedaan.<br>
  Als ik iemand tips kon geven voor reinforcement learning (mijzelf als ik naar het verleden kon gaan) dan zal ik als advies geven om met een zo klein mogelijk en simpel model te beginnen en dit steeds uit te breiden. Het is van belang dat hierbij gebruik wordt gemaakt van veel tutorials die zijn te vinden op het internet.<br>
  Wel vond ik dit een enorm interessant, maar uitdagend project, vooral omdat het heeft laten zien hoe ontzettend breed python kan zijn. En dat python niet alleen gebruikt kan worden voor het analyseren en visualiseren van een grote hoop aan data.<br><br>
  Omdat ik merkte dat ik reinforcement learning een lastig onderwerp vond, besloot ik daarom ook om mij vooral te ricthen op het research paper. Vandaar dat ik uiteindelijk de gehele inleiding had geschreven en het literatuuronderzoek heb uitgevoerd. Op deze manier kon ik mijn stekre punten zo goed mogelijk benutten tijdens het project. <br><br>
  Daarnaast zou ik ook willen onderzoeken op welke andere manieren dit containerprobleem opgeslost zou kunnen worden, denk hierbij aan regressiemodellen en linear programmeren. <br><br>
  Om de oplossing goed toe te kunnen passen bij een containerterminal zal het model ook opgeschaald moeten worden. Aan het einde van het onderzoek was het gelukt om een optimum te vinden voor grid van 3x3x1. Bij dit grid waren in totaal 9 mogelijke plekken voor een container, alleen werd er niet in de hoogte gestapeld. Dit zal moeten worden opgeschaald naar een grid ter grootte van een containerterminal. 
  Hier zijn vaak honderden tot duizenden plekken voor containers.
  Daarnaast werd in ons model gebruik gemaakt van van containers die naar 2 verschillende bestemmingen toe moesten, 4 voor elke boot. Op een terminal komen per dag veel meer boten binnen. Het is daarom ook van belang om dit te verwerken in het model.
  Om dit goed te kunnen verwerken zal er eerst onderzoek moeten worden gedaan naar de hoeveelheid schepen die gemiddeld per dag in en uit de containerterminal gaan en om hoeveel containers dat per dag zal gaan. Er kan dan een gemiddelde worden genomen voor een aantal grote havens in Nederland/ de wereld. Of er kan gerricht onderzoek worden uitgevoerd voor één containerterminal.
  
  <h4>Conclusions</h4>
  <h5>Model selecteren</h5>
  Aan de hand van het onderzoek dat is uitgevoerd naar de toepassing van de verschillende reinforcement moddellen :A2C en PPO, kwam PPO het beste naar voren op basis van de episodelengte en de resultaten. Aan de hand van het model kunnen containers nu op een efficiënte manier op de kade worden geplaatst. Dit leidt tot het besparen van tijd en dus ook geld.
  PPO kwam er beter uit, omdat de gemiddelde episodelengte korter was dan bij A2C. Dat is ook te zien in de volgende grafiek:
  <img src="https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Episodelengte.png"><br>

  <h5>Rewardfunctie</h5>
  Voor elke container die werd geplaatst op een beschikbare plek (waar op dat moment geen andere container is geplaatst) werd er een reward van 20 gegeven. Als er al een andere container op die plek stond, dan was er een penalty van -20. Als 2 containers naast elkaar kwamen te staan met dezelfde bestemming, dan werden er nog 10 extre punten toegekend. Als ze niet dezelfde bestemming deelden, dan werd er een penalty gegeven van -10.
  In de volgende grafiek is ook de gemiddelde episodereward per model gevisualiseerd. Hierin is ook te zien dat PPO sneller naar een hogere episodereward toe werkt.
  <img src="https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Episode%20reward.png"><br>
  <h5> Hyperparameters</h5>
  Voor de PPO agent zijn er 5 hyperparameters getuned (learning_rate, gamma, gae_lambda, ent_coef en vf_coef). Deze zijn getuned over een span van 500.000 trainingsstappen. Hierbij is er gekeken bij welke waarde van elke hyperparameter het optimum wordt bereikt met zo min mogelijk trainingsstappen.
  Aangezien dit nogal veel informatie bevat, verwijs ik u door naar het <a href ="https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Research%20Paper%20Container%20Project%20Groep%204.pdf"> research paper </a> (hoofdstuk 5.2)
  Er zal nog een vervolgonderzoek/ opschaling van het model moeten plaatsvinden. Er is nu namelijk gewerkt met een grid van 3x3x1 (3 breed, 3 lang en 1 hoog), waarop in totaal 8 containers worden geplaatst. Hierbij zijn er 9 plekken om een container te plaatsen. Bij een grootschalige haven zijn er natuurlijk veel meer plekken om een container neer te zetten, en meer containers om te plaatsen.
  <h5>Geoptimaliseerd PPO model</h5>
  Vervolgens is het PPO model getraind met de optimale hyperparameters, dit gaf de volgende gemiddelde episodelengte:
  <img src="https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/PPO%20episodelengte.png"><br>
  In deze grafiek is te zien dat er snel naar een optimum wordt gewerkt van 8 stappen, dit betekent dat voor elke container één stap nodig is, dit is optimaal aangezien er 8 containers zijn om te plaatsen.
  in de volgende grafiek is de gemiddelde episodereward te zien.
  <img src="https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/PPO%20episodereward.png"><br>
  Er is in de grafiek te zien dat er snel naar de optimale episodereward van 200 wordt toegewerkt. De theoretische beste score is namelijk 8 x 20 + 4 x 10 = 200.<br>
  Er kan worden geconcludeerd dat het huidige model optimaal getraind is op de environment en de rewardfunctie.
  Overegins zijn de complete conclusies ook terug te vinden in het <a href ="https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Research%20Paper%20Container%20Project%20Groep%204.pdf"> research paper </a>.
  <h4>Planning</h4>
  Tijdens dit project hebben we ook weer gewerkt met een <a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Scrum%20container.png'> scrum board</a>
  Tijdens het gebruik van dit scrum board is er gewerkt met sprints die elk 2 weken duurden, hierbij kreeg ieder groepslid taken om te voltooien tijdens deze sprint.
  Daarnaast waren er ook fysieke afspraken ingepland met de docenten. Om de maandag om 10:00 met Karin enlke vrijdag om 10:00 en 10:30 met Tony en Jeroen.
</details>
</details>

<details><summary><h2>Data preprocessing</h2></summary>
  Voor dit onderdeel heb ik zelf een dataset van kaggle gehaald, dat is <a href='https://www.kaggle.com/code/ahsan81/nyc-restaurant-food-order-delivery-detailed-eda'> deze </a> dataset. Dit heb ik gehaan omdat ik tijdens het foodboost project nog te weinig kennis had over python om deze taak op me te nemen en tijdens het containerproject had Akram de taak op zich genomen om dit uit te voeren. Het notebook, samen met comments, is <a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Phone_analysis%20(1).ipynb'>hier </a> te vinden.<br>
  Nu neem ik jullie mee in mijn data preprocessing:<br>
  Om mee te beginnen had ik de dataframe ingezien, vervogens heb ik het order id en customer id verwijderd van de dataset, aangezien ik hier geen verdere acties mee ging nemen.<br>
  Daarna controleerde ik of er bepaalde waarden misten in de dataset, dit was niet het geval, we waren sommige restaurantnamen verkeerd getypt, dit heb ik vervolgens aangepast. Dit heb ik ook gecontroleerd bij andere kolommen. Daarna heb ik alle kolommen met numerieke waarden ge-subset en deze vervolgens geplot om de data in de zien. Daarna heb ik gekeken naar de verdeling van de verschillende soorten keukens in de dataset.<br>
  Van deze keukens heb ik de twee meest voorkomende opnieuw ge-subset.
  
</details>
<details><summary><h2>Predictive Analytics</h2></summary>
  Voor dit onderdeel ben ik verder gegaan met de dataset van Kaggle, <a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Phone_analysis%20(1).ipynb'> dit </a> is de link naar mijn notebook.<br>
  Ik ging kijken of het mij zou lukken om een KNN classifier toe te passen om te voorspellen of een keuken amerikaans of japans kon zijn op basis van, de kosten, voorbereidingsduur en bezorgduur van een order. Na dit model gefit en getraind te hebben kwam, na al van tevoren verwacht te hebben, geen goede voorspelling hieruit. <br> Er kwam namelijk een score uit van 0,54%, het model kon dus met 54% zekerheid een voorspelling doen.<br><br>
  vervolgens ging ik aan de slag met het voorspellen een doordeweekse dag of een weekenddag. Hier is een knn classifier zeker geschikt voor, deze kan namelijk een voorspelling doen op basis van meerdere features in de dataset.<br>
  Eerst had ik een train_test_split uitgevoerd op mijn preprocessed data, hiermee kon ik mijn model valideren.
  Ik had eerst 6 neighbours ingesteld als basis, deze kon ik handmatig aanpassen. Uiteindelijk heb ik een graph gecodeerd waarin naar voren komt wat de optimale neighbours zijn voor het model
 ![image](https://user-images.githubusercontent.com/81196024/214803796-83597c6d-6be4-49ec-85e4-316b32f2b166.png)
Uit de tabel hierboven is te zien dat bij deze random state de optimale hoeveelheid neighbours 156 waren. Dit gaf een correcte voorspelling van %80 procent.<br>
  Daarna ben ik aan de slag gegaan met een cross validation model, hier kwam een score uit van gemiddeld 75,8%
 Vervolgens had ik een classificatierapprort gemaakt, hierin zijn de precision en recall terug te vinden.
                precision    recall  f1-score   support

     Weekday       0.86      0.27      0.41       110
     Weekend       0.77      0.98      0.86       270

    accuracy                           0.78       380
   macro avg       0.81      0.63      0.64       380
weighted avg       0.79      0.78      0.73       380
<br>
  het model kan voornamelijk goed een weekenddag voorspellen.
  </details>
<details><summary><h2>Communication</h2></summary>
Zelf ben ik een persoon die het niet erg vind om te presenteren, ik heb mijzelf ingezet voor vijf interne presentaties en ook nog twee externe presentaties. In totaal 7 presentaties waarbij ik voor elke presentatie meerdere slides heb gepresenteerd.
Dit zijn de presentaties die ik heb gepresenteerd:<br>
  Extern <br>
  <a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Presentaties/Foodboost%20eindpresentatie.pptx'>Externe presentatie week 6 </a>
<br>
<a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Presentaties/Externe%20presentatie%20week%2014%20groep%204.pptx'>Externe presentatie week 14 </a><br><br>
Intern<br>
<a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Presentaties/Interne%20presentatie%20week%202%20groep%204.pptx'>Interne presentatie week 2</a><br>
<a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Presentaties/Interne%20presentatie%20week%204%20groep%204.pptx'>Interne presentatie week 4</a><br>
<a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Presentaties/Interne%20presentatie%20week%206%20groep%204.pptx'>Interne presentatie week 6</a><br>
<a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Presentaties/Interne%20presentatie%20week%206%20groep%204.pptx'>Interne presentatie week 8</a><br>
<a href='https://github.com/Hessels070/Applied_Data_Science_18053580/blob/main/Presentaties/Interne%20presentatie%20week%206%20groep%204.pptx'>Interne presentatie week 12</a><br><br>
  Alles wat ik heb bijgedragen aan het research paper is terug te vinden in het hoofdstuk "research project".
</details><br>
